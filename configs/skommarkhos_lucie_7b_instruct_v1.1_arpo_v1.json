{
  "train_file": "",
  "eval_file": "",
  "use_nmt_callback": true,
  "lora": {
    "r": 32,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "bias": "none",
    "target_modules": [
      "lm_head",
      "q_proj",
      "v_proj",
      "k_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ]
  },
  "generation_config": {
    "max_new_tokens": 256,
    "temperature": 0.3,
    "top_p": 0.9,
    "do_sample": true,
    "num_beams": 3,
    "repetition_penalty": 1.3,
    "early_stopping": true
  },
  "max_tokens_count": 512,
  "max_length": 512,
  "eos_token": "<|eot_id|>",
  "pad_token": "<pad>",
  "bos_token": "<s>",
  "load_in_8bit": true,
  "load_in_4bit": false,
  "model_name": "OpenLLM-France/Lucie-7B-Instruct-v1.1",
  "response_template": "<|start_header_id|>assistant<|end_header_id|>\n\n",
  "trainer": {
    "dataset_num_proc": 2,
    "num_train_epochs": 2,
    "per_device_train_batch_size": 16,
    "per_device_eval_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "gradient_checkpointing": true,
    "learning_rate": 5e-7,
    "max_completion_length": 256,
    "max_prompt_length": 256,
    "logging_steps": 1,
    "max_grad_norm": 1.0,
    "lr_scheduler_type": "inverse_sqrt",
    "optim": "adamw_8bit",
    "loss_type": "simpo",
    "cpo_alpha": 0.0,
    "weight_decay": 0.01,
    "warmup_ratio": 0.01,
    "save_steps": 30,
    "save_total_limit": 1,
    "eval_steps": 10,
    "eval_strategy": "steps",
    "report_to": "wandb",
    "logging_dir": "./logs",
    "push_to_hub": true,
    "hub_model_id": "igorktech/skommarkhos-lucie-7b-instruct-v1.1-arpo-v1",
    "hub_private_repo": true
  },
  "output_dir": "/workspace/models/skommarkhos_lucie_7b_instruct_v1.1_arpo_v1",
  "seed": 3407
}
