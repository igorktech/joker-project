{
  "train_file": "data/task2/processed/train_sft_extended.json",
  "eval_file": "data/task2/processed/validation_sft.json",
  "use_nmt_callback": true,
  "lora": {
    "r": 32,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "bias": "none",
    "target_modules": [
      "lm_head",
      "q_proj",
      "v_proj",
      "k_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ]
  },
  "generation_config": {
    "max_new_tokens": 256,
    "temperature": 0.3,
    "top_p": 0.9,
    "do_sample": true,
    "num_beams": 3,
    "repetition_penalty": 1.3,
    "early_stopping": true
  },
  "max_tokens_count": 512,
  "max_length": 512,
  "eos_token": "<|eot_id|>",
  "pad_token": "<pad>",
  "bos_token": "<s>",
  "load_in_8bit": true,
  "load_in_4bit": false,
  "model_name": "OpenLLM-France/Lucie-7B-Instruct-v1.1",
  "completion_only": true,
  "response_template": "<|start_header_id|>assistant<|end_header_id|>\n\n",
  "trainer": {
    "num_train_epochs": 1,
    "per_device_train_batch_size": 16,
    "per_device_eval_batch_size": 8,
    "gradient_accumulation_steps": 4,
    "gradient_checkpointing": true,
    "learning_rate": 0.0001,
    "max_grad_norm": 1.0,
    "weight_decay": 0.01,
    "warmup_ratio": 0.01,
    "lr_scheduler_type": "inverse_sqrt",
    "optim": "adamw_8bit",
    "logging_steps": 1,
    "save_steps": 20,
    "eval_strategy": "steps",
    "eval_steps": 2,
    "save_total_limit": 1,
    "report_to": "wandb",
    "logging_dir": "./logs",
    "push_to_hub": true,
    "hub_model_id": "igorktech/skommarkhos-lucie7binstructv1-1-sft-v7",
    "hub_private_repo": true,
    "packing": false,
    "dataset_num_proc": 2
  },
  "output_dir": "/workspace/models/skommarkhos_lucie7binstructv1_1_sft_v7",
  "seed": 3407
}